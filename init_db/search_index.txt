-1 查看SQL查询的性能语句
参考链接
http://mysql.taobao.org/monthly/2018/11/06/
---探究索引效果
EXPLAIN ANALYZE SELECT book_id FROM search_book_intro WHERE tsv_column @@ '美丽';


0. 本机安装配置zh-parser
1. 配置zh-parser的环境
//可能只能superuser有配置这个extension的权利
CREATE EXTENSION zhparser;

CREATE TEXT SEARCH CONFIGURATION zh (PARSER = zhparser);
ALTER TEXT SEARCH CONFIGURATION zh ADD MAPPING FOR n,v,a,i,e,l,j WITH simple;
!> 注：当安装完毕之后 执行 select to_tsvector('zh', '好看女子千千万，一切如作白骨观'); 如果只是出现了查询结果为 t,却没有显示分词效果，就说明你少了 token 映射.
就说名你分词还是跑不了中文的检索,可能还是会有  NOTICE:  text-search query doesn't contain lexemes: ""的报错；
bookstore=#  SELECT to_tsvector('zh', '我爱 文因互联')
;
       to_tsvector
--------------------------
 '互联':3 '我爱':1 '文':2
(1 row)
形如这种才是正确的，才能往下
- 目前我的电脑上唯一能泡出来的中文检索方式:
- 执行search.py以后需要按照如下步骤操作:
- 我都测过了 可以执行SQL语句去测试
CREATE EXTENSION zhparser;
CREATE TEXT SEARCH CONFIGURATION zh (PARSER = zhparser);
ALTER TEXT SEARCH CONFIGURATION zh ADD MAPPING FOR n,v,a,i,e,l,j WITH simple;
 SELECT to_tsvector('zh', '我爱 文因互联')

 ALTER TABLE search_book_intro ADD COLUMN tsv_column tsvector; 
 UPDATE search_book_intro SET tsv_column = to_tsvector('zh', coalesce(book_intro''));   
 SELECT * from search_book_intro limit 1;//测试表结构列是否添加成功
 CREATE INDEX idx_gin_zh ON search_book_intro USING GIN(tsv_column); 
CREATE TRIGGER trigger_name BEFORE INSERT OR UPDATE  ON search_book_intro FOR EACH ROW EXECUTE PROCEDURE
tsvector_update_trigger(tsv_column, 'zh', book_intro);
UPDATE search_book_intro SET tsv_column = to_tsvector('zh', coalesce(book_intro,''));
 SELECT * from search_book_intro limit 1;
 ---

ALTER TABLE search_book_tags ADD COLUMN tsv_column tsvector;           
UPDATE search_book_tags SET tsv_column = to_tsvector('zh', coalesce(tags,''));   
CREATE INDEX idx_gin_tag ON search_book_tags USING GIN(tsv_column);  
CREATE TRIGGER trigger_name BEFORE INSERT OR UPDATE  ON search_book_tags FOR EACH ROW EXECUTE PROCEDURE
tsvector_update_trigger(tsv_column, 'zh', tags);
select * from search_book_tags limit 1;
SELECT * FROM search_book_tags WHERE tsv_column @@ '传记';
---

ALTER TABLE search_author ADD COLUMN tsv_column tsvector;           
UPDATE search_author SET tsv_column = to_tsvector('zh', coalesce(author,''));   
CREATE INDEX idx_gin_author ON search_author USING GIN(tsv_column);  
CREATE TRIGGER trigger_name BEFORE INSERT OR UPDATE  ON search_author FOR EACH ROW EXECUTE PROCEDURE
tsvector_update_trigger(tsv_column, 'zh', author);
SELECT * from search_author limit 20;
SELECT * FROM search_author WHERE tsv_column @@ '杨红';//这分词很影响准确率和召回率嘛:D
//用杨/杨红/杨红樱并不ok 
//依赖于zhparser人名也不ok
//最后用了2-gram去处理人名查出来了:D 也不会出现粒度分的太细的问题，因为这样杨红也是可以作为一个词被分词器返回的

ALTER TABLE search_title ADD COLUMN tsv_column tsvector;           
UPDATE search_title SET tsv_column = to_tsvector('zh', coalesce(title,''));   
CREATE INDEX idx_gin_title ON search_title USING GIN(tsv_column);  
CREATE TRIGGER trigger_name BEFORE INSERT OR UPDATE  ON search_title FOR EACH ROW EXECUTE PROCEDURE
tsvector_update_trigger(tsv_column, 'zh', title);
select * from search_title limit 1;
SELECT * FROM search_title WHERE tsv_column @@ '狗'; //这就是粒度太细造成查不出来的情况



 

3. author这种需要需要精细分词，比如香港特别行政区是分不出来的，分词结果直接影响我们的召回率
CREATE INDEX idx_gin_tsv ON search_book_intro USING GIN(to_tsvector('zhcnsearch', "book_intro"));
CREATE INDEX idx_gin_tsv ON search_book_tags USING GIN(to_tsvector('zhcnsearch', "tags"));
